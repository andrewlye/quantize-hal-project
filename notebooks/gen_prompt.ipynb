{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aly37/.conda/envs/lovelace/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading shards: 100%|██████████| 2/2 [00:59<00:00, 29.73s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.86s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/phi-2\", cache_dir='/home/aly37/.cache')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-2\", cache_dir='/home/aly37/.cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instruct: Generate a prompt that greatly increases the performance of a model in recalling factual information whose weights have been reduced to a lower precision. For context, the previous prompt: \"Please carefully examine the weight matrix within the model, as it may contain errors. It is crucial to verify its accuracy and make any necessary adjustments to ensure optimal performance in the following scenario. An exchange between a user and a helpful assistant that provides correct answers to the multiple-choice trivia questions the user asks.\" resulted in an accuracy gain of around 1 percent compared to baseline models. Your generated prompt should result in a greater improvement than the previous prompt. \n",
      "Output: Sure, here's a prompt that could potentially result in a greater improvement in accuracy:\n",
      "\"Please carefully analyze the weight matrix within the model, as it may contain errors that could negatively impact its performance. It is crucial to verify its accuracy and make any necessary adjustments to ensure optimal performance in the following scenario. An exchange between a user and a helpful assistant that provides correct answers to the multiple-choice trivia questions the user asks. However, this time, the assistant will also provide additional context and explanations for each answer, helping the user to better understand the material and retain the information for longer periods of time.\"\n",
      "<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer('Instruct: Generate a prompt that greatly increases the performance of a model in recalling factual information whose weights have been reduced to a lower precision. For context, the previous prompt: \"Please carefully examine the weight matrix within the model, as it may contain errors. It is crucial to verify its accuracy and make any necessary adjustments to ensure optimal performance in the following scenario. An exchange between a user and a helpful assistant that provides correct answers to the multiple-choice trivia questions the user asks.\" resulted in an accuracy gain of around 1 percent compared to baseline models. Your generated prompt should result in a greater improvement than the previous prompt. \\nOutput:',\n",
    "    return_tensors=\"pt\", return_attention_mask=False)\n",
    "\n",
    "outputs = model.generate(**inputs, max_length=500)\n",
    "text = tokenizer.batch_decode(outputs)[0]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lovelace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
